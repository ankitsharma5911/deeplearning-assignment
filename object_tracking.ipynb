{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is object tracking, and how does it differ from object detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Tracking  \n",
    "Object tracking is a computer vision technique that involves following a specific object or multiple objects across a sequence of video frames. It assumes the object is already detected in the first frame and focuses on maintaining its position and identity in subsequent frames.\n",
    "\n",
    "### Object Detection  \n",
    "Object detection is the process of identifying and locating objects within an image or a single frame of a video. It provides bounding boxes and class labels for objects but does not maintain their identity over time.\n",
    "\n",
    "### Key Differences  \n",
    "1. **Scope**:  \n",
    "   - Detection works on a frame-by-frame basis.  \n",
    "   - Tracking involves maintaining the trajectory of objects over multiple frames.  \n",
    "\n",
    "2. **Identity Maintenance**:  \n",
    "   - Detection doesn't assign persistent IDs to objects.  \n",
    "   - Tracking ensures objects retain consistent IDs across frames.  \n",
    "\n",
    "3. **Usage**:  \n",
    "   - Detection is the precursor to tracking.  \n",
    "   - Tracking is used for tasks like motion analysis and activity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is YOLO, and why is it popular for object detection in real-time application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **YOLO (You Only Look Once)**  \n",
    "YOLO is a real-time object detection algorithm that treats object detection as a single regression problem. It predicts the bounding boxes and class probabilities for objects in an image in one pass through the network.\n",
    "\n",
    "### **Why YOLO is Popular for Real-Time Applications**  \n",
    "1. **Speed**:  \n",
    "   - YOLO processes the entire image in one go, making it extremely fast.  \n",
    "   - Suitable for real-time applications like autonomous driving or video surveillance.  \n",
    "\n",
    "2. **Accuracy**:  \n",
    "   - Maintains good detection accuracy, especially for larger objects.  \n",
    "   - Optimized for both localization and classification tasks.  \n",
    "\n",
    "3. **Unified Architecture**:  \n",
    "   - Combines feature extraction, classification, and bounding box prediction into one network.  \n",
    "   - Simplifies deployment and reduces computational overhead.  \n",
    "\n",
    "4. **Scalability**:  \n",
    "   - Multiple versions (e.g., YOLOv4, YOLOv5) with improved performance and flexibility for different hardware.  \n",
    "\n",
    "Its balance of speed and accuracy makes YOLO a favorite for real-time object detection scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How does DeepSORT improve object tracking?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DeepSORT (Deep Simple Online and Realtime Tracking)** improves object tracking by combining traditional tracking methods with deep learning-based appearance modeling. Here's how it enhances tracking:\n",
    "\n",
    "### 1. **Association with Appearance Features**  \n",
    "- **Deep Learning Embeddings**: Uses a deep neural network to extract appearance features for each object, allowing it to distinguish between visually similar objects.  \n",
    "- This reduces ID switching by linking detections to existing tracks based on visual similarity and motion.\n",
    "\n",
    "### 2. **Motion Modeling with Kalman Filter**  \n",
    "- Tracks object motion using a Kalman Filter, predicting their positions in subsequent frames and accounting for movement patterns.  \n",
    "\n",
    "### 3. **Data Association with Hungarian Algorithm**  \n",
    "- Matches detections to existing tracks using both motion (position and velocity) and appearance features, improving the robustness of tracking.\n",
    "\n",
    "### 4. **Re-identification**  \n",
    "- If an object leaves the frame and re-enters, DeepSORT can re-identify it based on its appearance features, maintaining consistent IDs.\n",
    "\n",
    "### Key Advantages  \n",
    "- **Reduced ID Switching**: Appearance features improve accuracy in crowded scenes.  \n",
    "- **Robustness**: Handles occlusions and re-identifications effectively.  \n",
    "- **Scalability**: Works in real-time for many objects in dynamic environments.  \n",
    "\n",
    "By integrating motion and appearance cues, DeepSORT provides a more reliable and efficient tracking solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain the concept of state estimation in a Kalman Filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State estimation** in a Kalman Filter refers to the process of estimating the true state of a dynamic system (e.g., position, velocity) from noisy and uncertain measurements over time.\n",
    "\n",
    "### Key Concepts:\n",
    "1. **State Vector**: Represents the variables that describe the system's current state (e.g., position, velocity, etc.).\n",
    "2. **Prediction**: Based on the system's model, the Kalman Filter predicts the next state from the previous state estimate.\n",
    "3. **Update (Correction)**: The filter incorporates new measurements to refine the predicted state, adjusting it based on how reliable the prediction and the measurement are.\n",
    "\n",
    "### Steps in State Estimation:\n",
    "- **Prediction Step**:  \n",
    "  The filter predicts the next state based on a mathematical model (e.g., motion model) and the previous state. This step also predicts the uncertainty (covariance).\n",
    "  \n",
    "- **Update Step**:  \n",
    "  When a new measurement is available, the filter updates the predicted state by combining it with the measurement. The uncertainty of both the prediction and the measurement is considered, with more reliable information having more influence.\n",
    "\n",
    "### Goal:\n",
    "The objective of state estimation in Kalman Filter is to continuously produce the best estimate of the state by minimizing the error over time, even in the presence of noisy measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What are the challenges in object tracking across multiple frames?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object tracking across multiple frames faces several challenges:\n",
    "\n",
    "### 1. **Occlusion**  \n",
    "- Objects may become temporarily hidden behind other objects, making it difficult to track them accurately until they reappear.\n",
    "\n",
    "### 2. **Object Interaction**  \n",
    "- Objects may interact or overlap, causing confusion in distinguishing individual objects and maintaining consistent tracking.\n",
    "\n",
    "### 3. **Fast Motion**  \n",
    "- Objects moving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Describe the role of the Hungarian algorithm in DeepSORT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Hungarian Algorithm** plays a crucial role in **DeepSORT** by solving the **data association problem**, which involves matching detections to existing object tracks in each frame.\n",
    "\n",
    "### Key Role:\n",
    "- **Optimal Matching**: It assigns detections to tracks in a way that minimizes the overall cost, such as the distance between predicted and detected positions.  \n",
    "- **Cost Matrix**: The algorithm uses a cost matrix that combines motion (predicted position from Kalman Filter) and appearance features (deep learning embeddings).  \n",
    "- **Efficiency**: Ensures efficient and accurate association, even in crowded or dynamic scenes.\n",
    "\n",
    "### Outcome:\n",
    "By leveraging the Hungarian Algorithm, DeepSORT reliably links detections to tracks across frames, reducing mismatches and maintaining consistent object identities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What are the advantages of using YOLO over traditional object detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of YOLO over Traditional Object Detection Methods**:\n",
    "\n",
    "1. **Speed**:  \n",
    "   - YOLO processes the entire image in a single pass, making it much faster than traditional methods that rely on region proposals and multiple passes (e.g., R-CNN).  \n",
    "\n",
    "2. **End-to-End Architecture**:  \n",
    "   - Combines feature extraction, classification, and bounding box regression into a unified network, simplifying the workflow.\n",
    "\n",
    "3. **Real-Time Capability**:  \n",
    "   - Suitable for real-time applications like video surveillance, autonomous vehicles, and robotics due to its high processing speed.\n",
    "\n",
    "4. **Global Context Awareness**:  \n",
    "   - YOLO analyzes the entire image at once, making predictions with a better understanding of object context and relationships.\n",
    "\n",
    "5. **Efficiency on Smaller Datasets**:  \n",
    "   - Performs well even with smaller datasets, reducing the need for extensive training data.\n",
    "\n",
    "6. **Scalability**:  \n",
    "   - Adaptable for different performance needs, with versions like YOLOv5 optimized for lightweight deployment on edge devices.\n",
    "\n",
    "7. **Good Generalization**:  \n",
    "   - YOLO generalizes well to unseen images, reducing false positives compared to sliding-window or region-based methods.\n",
    "\n",
    "These advantages make YOLO a widely adopted approach for efficient and real-time object detection tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. How does the Kalman Filter handle uncertainty in predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Kalman Filter** handles uncertainty in predictions through the use of **covariance matrices** and a recursive update process:\n",
    "\n",
    "### 1. **Uncertainty Representation**  \n",
    "- **Error Covariance Matrix (P)**: Quantifies the uncertainty in the state estimate. A larger value indicates higher uncertainty.  \n",
    "- Both process noise (system model uncertainty) and measurement noise (sensor uncertainty) are modeled as Gaussian distributions.\n",
    "\n",
    "### 2. **Prediction Step**  \n",
    "- The Kalman Filter predicts the next state and updates the error covariance, increasing uncertainty due to process noise.  \n",
    "\n",
    "### 3. **Update Step**  \n",
    "- When a new measurement is received, the filter compares the predicted state to the measurement.  \n",
    "- The **Kalman Gain** determines how much the prediction and the measurement influence the updated estimate, balancing their uncertainties.  \n",
    "   - Reliable measurements (low noise) contribute more to the update.  \n",
    "   - Noisy measurements contribute less.\n",
    "\n",
    "### 4. **Recursive Adjustment**  \n",
    "- The filter recursively refines the state estimate and its uncertainty, reducing errors over time by combining predictions and measurements.  \n",
    "\n",
    "This iterative approach ensures that the Kalman Filter adapts dynamically to handle uncertainties in both the system and the measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. What is the difference between object tracking and object segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Object Tracking**  \n",
    "- **Goal**: Tracks the position and identity of objects across multiple frames in a video.  \n",
    "- **Output**: Bounding boxes or points that indicate the object's location over time.  \n",
    "- **Focus**: Maintaining object continuity and assigning consistent IDs across frames.  \n",
    "- **Application**: Surveillance, autonomous driving, and activity recognition.\n",
    "\n",
    "### **Object Segmentation**  \n",
    "- **Goal**: Identifies and delineates the exact pixels belonging to each object within an image or frame.  \n",
    "- **Output**: Pixel-level masks for each object, providing precise object boundaries.  \n",
    "- **Focus**: Understanding the shape and structure of objects within a single frame.  \n",
    "- **Application**: Image editing, medical imaging, and scene understanding.\n",
    "\n",
    "### **Key Differences**  \n",
    "1. **Granularity**:  \n",
    "   - Tracking focuses on object locations and identities.  \n",
    "   - Segmentation provides detailed object boundaries.  \n",
    "\n",
    "2. **Temporal Aspect**:  \n",
    "   - Tracking operates across frames (time-based).  \n",
    "   - Segmentation works on individual frames (spatial-based).  \n",
    "\n",
    "3. **Output Type**:  \n",
    "   - Tracking outputs IDs and trajectories.  \n",
    "   - Segmentation outputs pixel-level masks.  \n",
    "\n",
    "The two tasks can complement each other in applications requiring detailed object analysis over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. How can YOLO be used in combination with a Kalman Filter for tracking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining **YOLO** with a **Kalman Filter** enables robust object detection and tracking across video frames. Here's how they work together:\n",
    "\n",
    "### **1. Object Detection with YOLO**  \n",
    "- YOLO detects objects in each video frame, providing bounding boxes, class labels, and confidence scores.  \n",
    "- This serves as the input for the tracking process.\n",
    "\n",
    "### **2. Prediction with Kalman Filter**  \n",
    "- The Kalman Filter predicts the next position of each tracked object based on its previous state (e.g., position and velocity).  \n",
    "- This helps estimate object trajectories even in frames where detection might fail.\n",
    "\n",
    "### **3. Data Association**  \n",
    "- The detected objects from YOLO are matched to the predicted positions using a distance metric (e.g., Euclidean distance).  \n",
    "- Matching ensures that each detection is linked to the correct track, typically done using algorithms like the Hungarian Algorithm.\n",
    "\n",
    "### **4. State Update**  \n",
    "- Once a YOLO detection is matched to a Kalman Filter prediction, the filter updates the object's state (position, velocity) by incorporating the new detection.  \n",
    "- The Kalman Gain balances the influence of predictions and measurements, accounting for uncertainties.\n",
    "\n",
    "### **Advantages of the Combination**  \n",
    "- **Robustness**: The Kalman Filter handles occlusions and missed detections by maintaining predictions.  \n",
    "- **Accuracy**: YOLO provides reliable object detection, ensuring high-quality measurements for tracking.  \n",
    "- **Efficiency**: The combination allows real-time tracking with reduced computational overhead.  \n",
    "\n",
    "This synergy is ideal for applications like surveillance, autonomous driving, and sports analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. What are the key components of DeepSORT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DeepSORT** (Deep Simple Online and Realtime Tracking) builds upon SORT by integrating deep learning-based appearance features for improved tracking performance. Its key components include:\n",
    "\n",
    "### 1. **Kalman Filter**  \n",
    "- **Purpose**: Models object motion and predicts the next state (e.g., position and velocity).  \n",
    "- **Role**: Provides a robust framework for motion estimation and state prediction.\n",
    "\n",
    "### 2. **Deep Appearance Features**  \n",
    "- **Purpose**: Uses a deep neural network to extract unique appearance embeddings for objects.  \n",
    "- **Role**: Distinguishes between visually similar objects, reducing ID switches.  \n",
    "- **Training**: The embedding network is pre-trained for re-identification tasks.\n",
    "\n",
    "### 3. **Data Association**  \n",
    "- **Purpose**: Matches new detections with existing tracks.  \n",
    "- **Method**: Combines motion and appearance features in a cost matrix.  \n",
    "- **Algorithm**: Uses the Hungarian Algorithm to find optimal matches.\n",
    "\n",
    "### 4. **Track Management**  \n",
    "- **Purpose**: Manages the lifecycle of tracks.  \n",
    "- **Mechanisms**:  \n",
    "  - **Track Initialization**: Creates a new track when an unmatched detection occurs.  \n",
    "  - **Track Update**: Updates existing tracks with matched detections.  \n",
    "  - **Track Termination**: Deletes tracks that remain unmatched for a specified duration.\n",
    "\n",
    "### 5. **Bounding Box Association**  \n",
    "- **Purpose**: Relates detections (bounding boxes) from an object detector (e.g., YOLO) to active tracks.  \n",
    "- **Role**: Provides spatial data for tracking.\n",
    "\n",
    "### 6. **Re-Identification (ReID)**  \n",
    "- **Purpose**: Maintains consistent object IDs across occlusions and re-entries into the frame.  \n",
    "- **Mechanism**: Uses the appearance embedding to recognize previously tracked objects.\n",
    "\n",
    "### **Summary of Strengths**  \n",
    "- Combines motion and appearance features for robust tracking.  \n",
    "- Handles occlusions, re-identifications, and crowded scenes effectively.  \n",
    "- Suitable for real-time applications due to efficient design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Explain the process of associating detections with existing tracks in DeepSORT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **DeepSORT**, associating detections with existing tracks involves matching new detections to predicted track positions using both motion and appearance features. Here's the process in brief:\n",
    "\n",
    "#### 1. **Prediction**  \n",
    "- The **Kalman Filter** predicts the next position of each track based on its motion model.\n",
    "\n",
    "#### 2. **Cost Matrix Construction**  \n",
    "- A **cost matrix** is created using:  \n",
    "  - **Motion Distance**: Difference between predicted and detected bounding boxes (e.g., IoU or Mahalanobis distance).  \n",
    "  - **Appearance Distance**: Cosine distance between the deep feature embeddings of detections and tracks.\n",
    "\n",
    "#### 3. **Matching with Hungarian Algorithm**  \n",
    "- The **Hungarian Algorithm** is applied to the cost matrix to find the optimal match between detections and tracks, minimizing the overall cost.\n",
    "\n",
    "#### 4. **Update Tracks**  \n",
    "- Matched detections update the track's state and reset its \"age.\"  \n",
    "- Unmatched tracks are incrementally aged to handle occlusions.  \n",
    "- Unmatched detections are initialized as new tracks.\n",
    "\n",
    "This method ensures accurate tracking by integrating motion prediction and appearance-based re-identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Why is real-time tracking important in many applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-time tracking** is important because it enables immediate responses to dynamic changes, enhancing safety, efficiency, and user experience.  \n",
    "\n",
    "#### Key Applications:  \n",
    "1. **Safety**: Prevents accidents in autonomous vehicles, drones, and robotics.  \n",
    "2. **Security**: Detects and responds to threats in surveillance systems.  \n",
    "3. **Immersion**: Supports seamless interaction in AR/VR and gaming.  \n",
    "4. **Efficiency**: Optimizes traffic flow, logistics, and operations.  \n",
    "5. **Healthcare**: Facilitates timely interventions in patient monitoring and surgical systems.  \n",
    "\n",
    "Real-time tracking is critical for systems requiring quick decisions and precise control in dynamic environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Describe the prediction and update steps of a Kalman Filter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Kalman Filter** operates in two main steps: **Prediction** and **Update**.\n",
    "\n",
    "### **1. Prediction Step**  \n",
    "- **Purpose**: Predict the system's next state and its uncertainty based on the current state and motion model.  \n",
    "\n",
    "### **2. Update Step**  \n",
    "- **Purpose**: Correct the predicted state using the new measurement.  \n",
    "\n",
    "### **Summary**  \n",
    "- **Prediction**: Estimates the next state and uncertainty.  \n",
    "- **Update**: Refines the prediction using the new measurement and reduces uncertainty.  \n",
    "\n",
    "This iterative process ensures accurate and adaptive state estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. What is a bounding box, and how does it relate to object tracking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|A **bounding box** is a rectangular box used to define the location of an object in an image or video frame. It is typically represented by coordinates (e.g., top-left corner, width, and height).\n",
    "\n",
    "#### **Relation to Object Tracking**  \n",
    "1. **Object Localization**:  \n",
    "   - Bounding boxes are used to mark and track the position of objects across frames.\n",
    "\n",
    "2. **Input for Tracking Algorithms**:  \n",
    "   - Object detection models (e.g., YOLO) provide bounding boxes as inputs to tracking systems.\n",
    "\n",
    "3. **Association Across Frames**:  \n",
    "   - Object tracking matches bounding boxes in consecutive frames to maintain object identities over time.\n",
    "\n",
    "4. **Trajectory Analysis**:  \n",
    "   - By tracking bounding box movement, the system can analyze object motion paths.\n",
    "\n",
    "In summary, bounding boxes serve as the foundation for detecting, localizing, and tracking objects in dynamic scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. What is the purpose of combining object detection and tracking in a pipeline? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining **object detection** and **tracking** in a pipeline enhances performance and functionality in dynamic scenarios.  \n",
    "\n",
    "#### **Purpose**  \n",
    "1. **Improved Accuracy**:  \n",
    "   - Tracking refines detections by maintaining object identities over time, reducing false positives and missed detections.  \n",
    "\n",
    "2. **Temporal Consistency**:  \n",
    "   - Ensures consistent object IDs across frames, even during occlusions or motion.\n",
    "\n",
    "3. **Efficiency**:  \n",
    "   - Reduces the need for frame-by-frame detection, saving computational resources.  \n",
    "\n",
    "4. **Real-Time Capability**:  \n",
    "   - Tracking allows processing at higher frame rates by leveraging prior knowledge of object positions.  \n",
    "\n",
    "5. **Trajectory Analysis**:  \n",
    "   - Facilitates studying object movements for applications like behavior analysis, navigation, or forecasting.  \n",
    "\n",
    "6. **Enhanced Functionality**:  \n",
    "   - Enables features like object re-identification, path prediction, and interaction analysis.  \n",
    "\n",
    "By combining detection's localization and tracking's temporal linking, the pipeline becomes robust for real-world applications like surveillance, robotics, and autonomous systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                 Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a Kalman filter to predict and update the state of an object given its measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the Kalman Filter parameters\n",
    "def kalman_filter(z, x_prev, P_prev, F, B, u, H, R, Q):\n",
    "    \"\"\"\n",
    "    Kalman filter prediction and update steps.\n",
    "    \n",
    "    z : Measurement vector\n",
    "    x_prev : Previous state estimate\n",
    "    P_prev : Previous state covariance estimate\n",
    "    F : State transition matrix\n",
    "    B : Control input matrix\n",
    "    u : Control input vector\n",
    "    H : Measurement matrix\n",
    "    R : Measurement noise covariance\n",
    "    Q : Process noise covariance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prediction Step\n",
    "    x_pred = np.dot(F, x_prev) + np.dot(B, u)  # Predicted state estimate\n",
    "    P_pred = np.dot(np.dot(F, P_prev), F.T) + Q  # Predicted covariance estimate\n",
    "    \n",
    "    # Innovation (Measurement Residual)\n",
    "    y = z - np.dot(H, x_pred)  # Difference between actual and predicted measurement\n",
    "    \n",
    "    # Kalman Gain\n",
    "    S = np.dot(np.dot(H, P_pred), H.T) + R  # Innovation covariance\n",
    "    K = np.dot(np.dot(P_pred, H.T), np.linalg.inv(S))  # Kalman Gain\n",
    "    \n",
    "    # Update Step\n",
    "    x_updated = x_pred + np.dot(K, y)  # Updated state estimate\n",
    "    P_updated = P_pred - np.dot(np.dot(K, H), P_pred)  # Updated covariance estimate\n",
    "    \n",
    "    return x_updated, P_updated\n",
    "\n",
    "# Example Parameters\n",
    "x_prev = np.array([0, 0])  # Initial state (e.g., [position, velocity])\n",
    "P_prev = np.array([[1, 0], [0, 1]])  # Initial covariance matrix\n",
    "F = np.array([[1, 1], [0, 1]])  # State transition matrix (simple linear motion)\n",
    "B = np.array([[0.5], [1]])  # Control input matrix (for acceleration)\n",
    "u = np.array([0])  # No control input (can be acceleration)\n",
    "H = np.array([[1, 0]])  # Measurement matrix (we only measure position)\n",
    "R = np.array([[1]])  # Measurement noise covariance\n",
    "Q = np.array([[0.01, 0], [0, 0.01]])  # Process noise covariance\n",
    "\n",
    "# Example measurements (position)\n",
    "measurements = [1, 2, 3, 4, 5]  # Example position measurements\n",
    "\n",
    "# Kalman filter loop\n",
    "for z in measurements:\n",
    "    x_prev, P_prev = kalman_filter(np.array([z]), x_prev, P_prev, F, B, u, H, R, Q)\n",
    "    print(f\"Updated state: {x_prev}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function to normalize an image array such that pixel values are scaled between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"\n",
    "    Normalize an image array to scale pixel values between 0 and 1.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image array (e.g., shape (height, width, channels)).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Normalized image with pixel values between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Ensure the image array is of type float32 to avoid integer overflow\n",
    "    image = image.astype(np.float32)\n",
    "    \n",
    "    # Normalize the pixel values to the range [0, 1]\n",
    "    normalized_image = image / 255.0\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load an image using OpenCV (replace 'image.jpg' with your image file)\n",
    "image = cv2.imread('image.jpg')\n",
    "\n",
    "# Normalize the image\n",
    "normalized_image = normalize_image(image)\n",
    "\n",
    "# Print the result\n",
    "print(normalized_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a function to generate dummy object detection data with confidence scores and bounding boxes. Filter the detections based on a confidence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_and_filter_detections(confidence_threshold=0.5, num_detections=10):\n",
    "    \"\"\"\n",
    "    Generate dummy object detection data with confidence scores and bounding boxes,\n",
    "    and filter detections based on a confidence threshold.\n",
    "    \n",
    "    Parameters:\n",
    "        confidence_threshold (float): The threshold above which detections are kept.\n",
    "        num_detections (int): The number of dummy detections to generate.\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered list of bounding boxes and confidence scores.\n",
    "    \"\"\"\n",
    "    # Generate dummy bounding boxes (x1, y1, x2, y2) in the range [0, 1]\n",
    "    bounding_boxes = np.random.rand(num_detections, 4)\n",
    "    \n",
    "    # Generate random confidence scores between 0 and 1\n",
    "    confidence_scores = np.random.rand(num_detections)\n",
    "    \n",
    "    # Filter detections based on the confidence threshold\n",
    "    filtered_detections = [(bbox, score) for bbox, score in zip(bounding_boxes, confidence_scores) if score >= confidence_threshold]\n",
    "    \n",
    "    return filtered_detections\n",
    "\n",
    "# Generate and filter detections with a confidence threshold of 0.7\n",
    "filtered_detections = generate_and_filter_detections(confidence_threshold=0.7)\n",
    "\n",
    "# Print the filtered detections\n",
    "for bbox, score in filtered_detections:\n",
    "    print(f\"Bounding Box: {bbox}, Confidence: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a function that takes a list of YOLO detections and extracts a random 128-dimensional feature vector for each detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_random_features(detections, feature_dim=128):\n",
    "    \"\"\"\n",
    "    Extract a random 128-dimensional feature vector for each YOLO detection.\n",
    "    \n",
    "    Parameters:\n",
    "        detections (list): List of detections, where each detection is a tuple (bounding_box, confidence_score).\n",
    "        feature_dim (int): The dimensionality of the feature vector (default is 128).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of feature vectors for each detection.\n",
    "    \"\"\"\n",
    "    feature_vectors = []\n",
    "    \n",
    "    for detection in detections:\n",
    "        # For each detection, generate a random feature vector of the specified dimensionality\n",
    "        feature_vector = np.random.rand(feature_dim)\n",
    "        feature_vectors.append(feature_vector)\n",
    "    \n",
    "    return feature_vectors\n",
    "\n",
    "# Example YOLO detections (bounding box and confidence score)\n",
    "detections = [\n",
    "    (np.array([0.1, 0.2, 0.5, 0.6]), 0.9),\n",
    "    (np.array([0.3, 0.4, 0.7, 0.8]), 0.8),\n",
    "    (np.array([0.5, 0.6, 0.9, 1.0]), 0.95)\n",
    "]\n",
    "\n",
    "# Extract 128-dimensional feature vectors for each detection\n",
    "feature_vectors = extract_random_features(detections)\n",
    "\n",
    "# Print the feature vectors\n",
    "for idx, feature_vector in enumerate(feature_vectors):\n",
    "    print(f\"Detection {idx+1}: Feature Vector: {feature_vector[:10]}...\")  # Showing first 10 elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a function to re-identify objects by matching feature vectors based on Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def reidentify_objects(detections, feature_vectors, distance_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Re-identify objects by matching feature vectors based on Euclidean distance.\n",
    "    \n",
    "    Parameters:\n",
    "        detections (list): List of detections, where each detection is a tuple (bounding_box, confidence_score).\n",
    "        feature_vectors (list): List of feature vectors corresponding to each detection.\n",
    "        distance_threshold (float): Maximum Euclidean distance to consider for matching objects.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping detection indices to re-identified object IDs.\n",
    "    \"\"\"\n",
    "    object_ids = {}\n",
    "    next_id = 0\n",
    "    \n",
    "    \n",
    "    distances = cdist(feature_vectors, feature_vectors, metric='euclidean')\n",
    "    \n",
    "    for i in range(len(detections)):\n",
    "       \n",
    "        matched = False\n",
    "        for j in range(i):\n",
    "            if distances[i, j] < distance_threshold:\n",
    "                object_ids[i] = object_ids[j]  \n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            object_ids[i] = next_id  # Assign a new ID if no match found\n",
    "            next_id += 1\n",
    "    \n",
    "    return object_ids\n",
    "\n",
    "\n",
    "feature_vectors = [\n",
    "    np.random.rand(128),\n",
    "    np.random.rand(128),\n",
    "    np.random.rand(128)\n",
    "]\n",
    "\n",
    "# Example detections (bounding box and confidence score)\n",
    "detections = [\n",
    "    (np.array([0.1, 0.2, 0.5, 0.6]), 0.9),\n",
    "    (np.array([0.3, 0.4, 0.7, 0.8]), 0.85),\n",
    "    (np.array([0.5, 0.6, 0.9, 1.0]), 0.95)\n",
    "]\n",
    "\n",
    "# Re-identify objects\n",
    "object_ids = reidentify_objects(detections, feature_vectors, distance_threshold=0.5)\n",
    "\n",
    "# Print the assigned object IDs\n",
    "print(object_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function to track object positions using YOLO detections and a Kalman Filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class ObjectTracker:\n",
    "    def __init__(self):\n",
    "        self.trackers = []\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def initialize_kalman(self, bbox):\n",
    "        \"\"\" Initialize Kalman Filter for a new object \"\"\"\n",
    "        kf = KalmanFilter(dim_x=4, dim_z=2)  # state [x, y, vx, vy], measurement [x, y]\n",
    "        kf.F = np.array([[1, 0, 1, 0],  # Transition matrix\n",
    "                         [0, 1, 0, 1],\n",
    "                         [0, 0, 1, 0],\n",
    "                         [0, 0, 0, 1]])\n",
    "        kf.H = np.array([[1, 0, 0, 0],  # Measurement matrix\n",
    "                         [0, 1, 0, 0]])\n",
    "        kf.R = np.array([[10, 0],  # Measurement noise covariance\n",
    "                         [0, 10]])\n",
    "        kf.Q = np.eye(4) * 0.1  # Process noise covariance\n",
    "        kf.x = np.array([bbox[0], bbox[1], 0, 0])  # Initial position and velocity\n",
    "        kf.P = np.eye(4) * 10  # Initial state covariance\n",
    "        return kf\n",
    "    \n",
    "    def update(self, detections):\n",
    "        \"\"\" Update the Kalman filters with new detections \"\"\"\n",
    "        # Extract the detected positions\n",
    "        detected_positions = np.array([detection[0] for detection in detections])\n",
    "        \n",
    "        if len(self.trackers) == 0:\n",
    "            # Initialize new trackers if there are no existing ones\n",
    "            for bbox in detected_positions:\n",
    "                kf = self.initialize_kalman(bbox)\n",
    "                self.trackers.append(kf)\n",
    "                self.next_id += 1\n",
    "        else:\n",
    "            # Predict the new state of all existing trackers\n",
    "            predicted_positions = []\n",
    "            for tracker in self.trackers:\n",
    "                tracker.predict()\n",
    "                predicted_positions.append(tracker.x[:2])\n",
    "            \n",
    "            predicted_positions = np.array(predicted_positions)\n",
    "            \n",
    "            # Calculate the Euclidean distance between predicted and detected positions\n",
    "            distances = cdist(predicted_positions, detected_positions, metric='euclidean')\n",
    "            \n",
    "            # Assign new detections to the closest trackers\n",
    "            for i, detection in enumerate(detected_positions):\n",
    "                min_distance_idx = np.argmin(distances[:, i])\n",
    "                self.trackers[min_distance_idx].update(detection)\n",
    "        \n",
    "        # Return the updated object positions (IDs, positions)\n",
    "        return [(self.trackers[i].x[:2], i) for i in range(len(self.trackers))]\n",
    "\n",
    "# Example Usage\n",
    "tracker = ObjectTracker()\n",
    "\n",
    "# Example YOLO detections (bounding boxes for each detection)\n",
    "detections = [\n",
    "    (np.array([100, 150]), 0.9),  # [x, y] position with confidence\n",
    "    (np.array([200, 250]), 0.8)\n",
    "]\n",
    "\n",
    "# Update the tracker with new detections\n",
    "tracked_objects = tracker.update(detections)\n",
    "\n",
    "# Print the tracked object IDs and their updated positions\n",
    "for obj_id, position in tracked_objects:\n",
    "    print(f\"Object ID: {obj_id}, Position: {position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Implement a simple Kalman Filter to track an object's position in a 2D space (simulate the object's movement with random noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self):\n",
    "        # State vector: [x, y, vx, vy]\n",
    "        self.x = np.zeros(4)  # Initial state (position and velocity)\n",
    "        \n",
    "        # State transition matrix (A)\n",
    "        self.F = np.array([[1, 0, 1, 0],  # x position update\n",
    "                           [0, 1, 0, 1],  # y position update\n",
    "                           [0, 0, 1, 0],  # velocity x update\n",
    "                           [0, 0, 0, 1]]) # velocity y update\n",
    "        \n",
    "        # Measurement matrix (H)\n",
    "        self.H = np.array([[1, 0, 0, 0],  # We only measure position\n",
    "                           [0, 1, 0, 0]])\n",
    "        \n",
    "        # Process noise covariance (Q)\n",
    "        self.Q = np.eye(4) * 0.1  # Assume small process noise\n",
    "        \n",
    "        # Measurement noise covariance (R)\n",
    "        self.R = np.eye(2) * 1.0  # Assume moderate measurement noise\n",
    "        \n",
    "        # Initial estimate covariance (P)\n",
    "        self.P = np.eye(4) * 10.0\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\" Predict the next state \"\"\"\n",
    "        self.x = np.dot(self.F, self.x)\n",
    "        self.P = np.dot(np.dot(self.F, self.P), self.F.T) + self.Q\n",
    "    \n",
    "    def update(self, z):\n",
    "        \"\"\" Update the state with a new measurement \"\"\"\n",
    "        y = z - np.dot(self.H, self.x)  # Innovation or residual\n",
    "        S = np.dot(np.dot(self.H, self.P), self.H.T) + self.R  # Residual covariance\n",
    "        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))  # Kalman gain\n",
    "        \n",
    "        self.x = self.x + np.dot(K, y)  # Update state estimate\n",
    "        self.P = self.P - np.dot(np.dot(K, self.H), self.P)  # Update covariance estimate\n",
    "\n",
    "def simulate_movement(num_steps=50):\n",
    "    \"\"\" Simulate the movement of the object with random noise \"\"\"\n",
    "    true_positions = []\n",
    "    measurements = []\n",
    "    \n",
    "    # Initial true position and velocity\n",
    "    true_position = np.array([0, 0, 1, 1])  # [x, y, vx, vy]\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        # Simulate true movement\n",
    "        true_position[0] += true_position[2]  # x = x + vx\n",
    "        true_position[1] += true_position[3]  # y = y + vy\n",
    "        \n",
    "        # Add random noise to simulate measurement noise\n",
    "        noise = np.random.randn(2)  # 2D Gaussian noise\n",
    "        measurement = true_position[:2] + noise  # Only measure x, y position\n",
    "        \n",
    "        true_positions.append(true_position[:2])\n",
    "        measurements.append(measurement)\n",
    "    \n",
    "    return np.array(true_positions), np.array(measurements)\n",
    "\n",
    "# Initialize Kalman Filter and simulate object movement\n",
    "kf = KalmanFilter()\n",
    "true_positions, measurements = simulate_movement(num_steps=50)\n",
    "\n",
    "# Apply Kalman Filter to track the object\n",
    "estimated_positions = []\n",
    "for z in measurements:\n",
    "    kf.predict()\n",
    "    kf.update(z)\n",
    "    estimated_positions.append(kf.x[:2])\n",
    "\n",
    "# Plot the true positions, measurements, and Kalman Filter estimates\n",
    "true_positions = np.array(true_positions)\n",
    "estimated_positions = np.array(estimated_positions)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(true_positions[:, 0], true_positions[:, 1], label='True Position', color='g', linestyle='-', marker='o')\n",
    "plt.plot(measurements[:, 0], measurements[:, 1], label='Noisy Measurements', color='r', linestyle=':', marker='x')\n",
    "plt.plot(estimated_positions[:, 0], estimated_positions[:, 1], label='Kalman Filter Estimate', color='b', linestyle='-', marker='.')\n",
    "plt.legend()\n",
    "plt.title('Kalman Filter Object Tracking in 2D Space')\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
