{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. What types of tasks does Detectron2 support?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detectron2** is a popular open-source library developed by Facebook AI Research (FAIR) for object detection, segmentation, and other computer vision tasks. It is a PyTorch-based framework that provides state-of-the-art implementations of various deep learning models for image recognition tasks.\n",
    "\n",
    "**Key Features of Detectron2:**\n",
    "\n",
    "1. **Object Detection**:\n",
    "   - Detectron2 supports models for **object detection**, which involves identifying and classifying objects within an image and drawing bounding boxes around them.\n",
    "\n",
    "2. **Instance Segmentation**:\n",
    "   - It also supports **instance segmentation**, which goes beyond object detection by not only detecting objects but also providing pixel-wise segmentation masks for each object.\n",
    "\n",
    "3. **Keypoint Detection**:\n",
    "   - Detectron2 can be used for detecting keypoints in images, which is useful in applications like human pose estimation.\n",
    "\n",
    "4. **Semantic Segmentation**:\n",
    "   - It also supports **semantic segmentation**, where each pixel in an image is classified as belonging to a particular class.\n",
    "\n",
    "5. **Flexible and Modular Design**:\n",
    "   - Detectron2 is designed to be highly flexible and modular. It allows easy configuration of models, data pipelines, and evaluation metrics.\n",
    "\n",
    "6. **Support for Advanced Models**:\n",
    "   - The library includes implementations of several advanced architectures like **Faster R-CNN**, **Mask R-CNN**, **RetinaNet**, **Cascade R-CNN**, **DensePose**, and more.\n",
    "\n",
    "7. **Easy Training and Inference**:\n",
    "   - It simplifies the process of training and evaluating models. Detectron2 comes with pre-trained models and utilities for fine-tuning on custom datasets.\n",
    "\n",
    "8. **GPU Acceleration**:\n",
    "   - The library is optimized for **GPU** use, making it highly efficient for large-scale training and inference tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why is data annotation important when training object detection models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data annotation is crucial for training object detection models because it provides the labeled data needed to teach models to recognize and localize objects. High-quality annotations, including accurate bounding boxes and class labels, ensure the model learns effectively, handles edge cases, and performs well in diverse scenarios. Without proper annotation, models cannot generalize, leading to poor detection accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What does batch size refer to in the context of model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size refers to the number of training examples processed simultaneously before updating the model's weights during training. It determines how many samples are passed through the model in one forward and backward pass, impacting training speed, memory usage, and convergence stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the purpose of pretrained weights in object detection models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained weights in object detection models provide a starting point by leveraging knowledge learned from large datasets (e.g., ImageNet). This helps improve accuracy, reduces training time, and requires less labeled data, as the model has already learned general features like edges, shapes, and textures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How can you verify that Detectron2 was installed correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that **Detectron2** was installed correctly by running the following Python commands:\n",
    "\n",
    "1. **Import Detectron2**:\n",
    "   ```python\n",
    "   import detectron2\n",
    "   print(\"Detectron2 imported successfully!\")\n",
    "   ```\n",
    "\n",
    "2. **Check Version**:\n",
    "   ```python\n",
    "   import detectron2\n",
    "   print(detectron2.__version__)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is TFOD2, and why is it widely used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFOD2 (TensorFlow Object Detection API 2)** is an open-source framework built on TensorFlow 2 for developing, training, and deploying object detection models. \n",
    "\n",
    "### **Why Itâ€™s Widely Used:**\n",
    "1. **Pretrained Models:** Provides access to a large collection of pretrained models (e.g., SSD, Faster R-CNN).\n",
    "2. **Flexibility:** Supports custom training for a wide range of applications.\n",
    "3. **Ease of Use:** Offers prebuilt pipelines for data preparation, training, and evaluation.\n",
    "4. **Active Community:** Backed by TensorFlow with extensive documentation and community support.\n",
    "5. **Production-Ready:** Enables deployment on various platforms like mobile, edge devices, and cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How does learning rate affect model training in Detectron2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Detectron2**, the learning rate controls how much the model's weights are adjusted during training in response to the calculated gradients. It directly influences the speed and quality of convergence. Here's how it impacts training:\n",
    "\n",
    "1. **Too High**: If the learning rate is too high, the model might overshoot the optimal point, leading to unstable training and poor convergence.\n",
    "2. **Too Low**: If the learning rate is too low, the model will learn very slowly, potentially getting stuck in suboptimal solutions or taking too long to converge.\n",
    "3. **Optimal Rate**: A well-chosen learning rate leads to faster convergence without overshooting, allowing the model to learn efficiently.\n",
    "\n",
    "In practice, learning rates are often tuned using strategies like learning rate schedules or adaptive optimizers (e.g., Adam) for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Why might Detectron2 use PyTorch as its backend framework?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detectron2 uses **PyTorch** as its backend framework because of several key advantages:\n",
    "\n",
    "1. **Dynamic Computation Graphs**: PyTorch offers dynamic computation graphs, which makes debugging easier and allows for more flexible model architectures.\n",
    "2. **Efficient GPU Support**: PyTorch has robust support for GPUs, which is essential for the high computational demands of object detection models in Detectron2.\n",
    "3. **Ease of Use**: PyTorch's intuitive interface and Pythonic design make it easier for researchers and developers to implement and experiment with new models and algorithms.\n",
    "4. **Extensive Ecosystem**: PyTorch has a rich ecosystem of libraries, tools, and community resources, which can accelerate development and integration with other tasks like transfer learning and data augmentation.\n",
    "5. **Performance and Scalability**: PyTorch is optimized for performance, particularly in deep learning tasks, and scales efficiently across multiple GPUs, making it ideal for large-scale model training and deployment.\n",
    "\n",
    "These factors make PyTorch a strong foundation for building and optimizing advanced computer vision models like those in Detectron2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What types of pretrained models does TFOD2 support?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFOD2** (TensorFlow Object Detection API 2) supports several types of pretrained models for various object detection tasks. These models are based on different backbone architectures and detection heads. Here are the main types:\n",
    "\n",
    "1. **SSD (Single Shot Multibox Detector)**: A fast and efficient model for real-time object detection. It supports backbones like MobileNet, Inception, and ResNet.\n",
    "2. **Faster R-CNN**: A more accurate model but slower than SSD, using Region Proposal Networks (RPN) for detecting objects. It supports backbones like ResNet and Inception.\n",
    "3. **EfficientDet**: A more efficient model that balances speed and accuracy using EfficientNet as the backbone.\n",
    "4. **RetinaNet**: A single-stage detector with a focus on solving the class imbalance problem with the Focal Loss function.\n",
    "5. **YOLO (You Only Look Once)**: A fast, single-stage detector optimized for real-time performance.\n",
    "\n",
    "These pretrained models, available in TFOD2, help in transferring learning for various object detection tasks, saving time and computational resources in training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. How can data path errors impact Detectron2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data path errors in **Detectron2** can have several negative impacts on model training and evaluation:\n",
    "\n",
    "1. **Data Loading Failures**: Incorrect or missing file paths can prevent the dataset from being loaded, leading to errors during training or evaluation.\n",
    "2. **Inconsistent Data**: If the paths are incorrect, the model might receive corrupted or incomplete data, resulting in poor training performance and inaccurate predictions.\n",
    "3. **Mismatch in Format**: If the data path points to incorrectly formatted datasets or annotations, it can cause parsing errors and prevent the model from interpreting the data correctly.\n",
    "4. **Training Disruption**: Inconsistent or missing data paths can interrupt the training process, causing crashes or halting the model from running, which could lead to wasted training time.\n",
    "\n",
    "To avoid these issues, it's crucial to double-check data paths and ensure proper directory structures when working with Detectron2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What is Detectron2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detectron2** is an open-source, high-performance library developed by Facebook AI Research (FAIR) for object detection tasks. It provides implementations of various state-of-the-art models for tasks like **object detection**, **instance segmentation**, **keypoint detection**, and **panoptic segmentation**. Built on top of **PyTorch**, Detectron2 is modular and flexible, allowing easy customization and extension of models. It supports training and inference with pretrained models, enabling fast deployment and experimentation with different architectures like Faster R-CNN, Mask R-CNN, and RetinaNet. Detectron2 is widely used in research and production for computer vision applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. What are TFRecord files, and why are they used in TFOD2 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFRecord** files are a data format used by TensorFlow for storing and efficiently reading large datasets. They are a binary file format that wraps structured data (such as images, labels, and annotations) into a single file, making it easier to handle large-scale datasets during training and evaluation.\n",
    "\n",
    "In **TFOD2**, TFRecord files are used for several reasons:\n",
    "1. **Efficiency**: TFRecord allows efficient reading and writing of large datasets, particularly when dealing with images and their annotations.\n",
    "2. **Scalability**: TFRecord files can handle large datasets by storing them in a compact binary format, which improves training speed and reduces I/O overhead.\n",
    "3. **Compatibility**: TFRecord is natively supported by TensorFlow, making it a convenient format for training object detection models in the TFOD2 pipeline.\n",
    "\n",
    "By using TFRecord files, TFOD2 can efficiently manage large-scale datasets while ensuring high performance during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. What evaluation metrics are typically used with Detectron2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Detectron2**, the following evaluation metrics are commonly used for object detection tasks:\n",
    "\n",
    "1. **Average Precision (AP)**:\n",
    "   - **AP@IoU=0.5 (AP50)**: Measures the average precision when the Intersection over Union (IoU) threshold is set to 0.5. This is the standard AP metric.\n",
    "   - **AP@IoU=0.75 (AP75)**: Measures precision with a stricter IoU threshold of 0.75.\n",
    "   - **AP (mAP)**: Mean Average Precision, which averages the AP over multiple IoU thresholds (e.g., from 0.5 to 0.95 in steps of 0.05).\n",
    "   \n",
    "2. **Precision**: The fraction of true positive detections out of all positive detections (including false positives).\n",
    "\n",
    "3. **Recall**: The fraction of true positive detections out of all ground truth objects.\n",
    "\n",
    "4. **IoU (Intersection over Union)**: Measures the overlap between predicted bounding boxes and ground truth boxes. Higher IoU indicates better performance.\n",
    "\n",
    "5. **F1 Score**: The harmonic mean of precision and recall, providing a single metric to balance both.\n",
    "\n",
    "These metrics help assess the quality of object detection models in terms of accuracy, precision, and recall, guiding model optimization and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. How do you perform inference with a trained Detectron2 model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform inference with a trained **Detectron2** model, follow these steps:\n",
    "\n",
    "1. **Install Detectron2**: Ensure you have Detectron2 and its dependencies installed.\n",
    "\n",
    "2. **Load the Configuration**: Load the configuration file used during training, or use a pre-trained model's config for inference.\n",
    " \n",
    "3. **Set the Model Weights**: Load the trained model's weights (e.g., from a checkpoint file).\n",
    "\n",
    "\n",
    "4. **Create the Model**: Initialize the model using the config.\n",
    "\n",
    "\n",
    "5. **Run Inference**: Pass an image to the model's predictor for inference.\n",
    "\n",
    "\n",
    "6. **View Results**: The `outputs` contain predictions such as detected bounding boxes, masks, and class labels. You can visualize them with `detectron2.utils.visualizer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. What does TFOD2 stand for, and what is it designed for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFOD2** stands for **TensorFlow Object Detection API 2**. It is an open-source library designed for building, training, and deploying object detection models in TensorFlow. TFOD2 provides a set of pre-trained models and tools to easily create, train, and evaluate object detection models on custom datasets. It supports various architectures like **Faster R-CNN**, **SSD**, **EfficientDet**, and **RetinaNet**, and offers utilities for handling data, training, and evaluation. TFOD2 is widely used for tasks like **object detection**, **instance segmentation**, and **keypoint detection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. What does fine-tuning pretrained weights involve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fine-tuning pretrained weights involves taking a model that has already been trained on a large dataset (pretrained) and adapting it to perform well on a specific, usually smaller, target task. \n",
    "\n",
    "**Key steps include:**\n",
    "\n",
    "1. **Initialization:** Start with the pretrained model weights.\n",
    "2. **Customization:** Modify the model architecture if necessary (e.g., add task-specific layers like classification heads).\n",
    "3. **Training on New Data:** Train the model on the target task dataset, typically with a lower learning rate to retain learned general features while adapting to the new task.\n",
    "4. **Optimization:** Use regularization techniques like early stopping or dropout to avoid overfitting, especially when the target dataset is small.\n",
    "\n",
    "Fine-tuning leverages the general knowledge encoded in pretrained models to improve performance on specialized tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. How is training started in TFOD2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training in TensorFlow Object Detection API (TFOD2) involves the following steps:\n",
    "\n",
    "1. **Install Dependencies:** Ensure TensorFlow, TFOD2 API, and other required libraries are installed.\n",
    "\n",
    "2. **Prepare Dataset:** Format your dataset in TFRecord format and update the label map file with class labels.\n",
    "\n",
    "3. **Select Model and Config:** Choose a pretrained model from the TFOD2 Model Zoo and download its configuration file.\n",
    "\n",
    "4. **Modify Config File:** Update the paths for:\n",
    "   - Training and validation TFRecord files.\n",
    "   - Label map file.\n",
    "   - Model checkpoint (for transfer learning).\n",
    "   - Hyperparameters like learning rate, batch size, and number of steps.\n",
    "\n",
    "5. **Launch Training Script:** Use the `model_main_tf2.py` script to start training:\n",
    "   ```bash\n",
    "   python model_main_tf2.py --model_dir=PATH_TO_MODEL_DIR --pipeline_config_path=PATH_TO_PIPELINE_CONFIG --num_train_steps=NUM_STEPS\n",
    "   ```\n",
    "\n",
    "6. **Monitor Training:** Use TensorBoard to track metrics like loss during training.\n",
    "\n",
    "Training begins, and the model learns to detect objects in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. What does COCO format represent, and why is it popular in Detectron2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COCO format is a widely-used data annotation format designed for object detection, segmentation, and keypoint detection tasks. It is structured as a JSON file containing information about images, annotations, and categories.\n",
    "\n",
    "**Key components of COCO format:**\n",
    "1. **Images:** Metadata about images, such as IDs, file names, and dimensions.\n",
    "2. **Annotations:** Bounding boxes, segmentation masks, keypoints, and their corresponding image IDs.\n",
    "3. **Categories:** Labels and IDs for different object classes.\n",
    "\n",
    "**Why it's popular in Detectron2:**\n",
    "- **Standardization:** COCO is an industry-standard, simplifying dataset sharing and use across tools and libraries.\n",
    "- **Compatibility:** Detectron2 natively supports COCO, making it easy to train models with minimal preprocessing.\n",
    "- **Rich Features:** Supports various tasks (e.g., instance segmentation, keypoint detection), aligning with Detectron2â€™s capabilities.\n",
    "- **Pretrained Models:** Many Detectron2 pretrained models are trained on the COCO dataset, facilitating transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Why is evaluation curve plotting important in Detectron2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation curve plotting in Detectron2 is important because it provides visual insights into the model's performance and helps in fine-tuning. Key reasons include:\n",
    "\n",
    "1. **Performance Tracking:** Shows metrics like accuracy, precision, recall, and mAP over training epochs or iterations.\n",
    "2. **Overfitting/Underfitting Detection:** Identifies if the model performs well on the training set but poorly on the validation set (overfitting) or struggles on both (underfitting).\n",
    "3. **Hyperparameter Optimization:** Assists in tuning learning rate, batch size, or regularization by analyzing metric trends.\n",
    "4. **Debugging:** Helps identify issues like learning stagnation or unexpected performance drops.\n",
    "\n",
    "Common curves include loss vs. iterations, precision-recall curves, and mAP trends, which guide model improvement decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. How do you configure data paths in TFOD2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure data paths in TensorFlow Object Detection API (TFOD2), update the **`pipeline.config`** file for your model:\n",
    "\n",
    "1. **TFRecord Paths:**\n",
    "   - Specify paths to training and validation TFRecord files:\n",
    "     ```protobuf\n",
    "     train_input_reader {\n",
    "       tf_record_input_reader {\n",
    "         input_path: \"path/to/train.record\"\n",
    "       }\n",
    "     }\n",
    "     eval_input_reader {\n",
    "       tf_record_input_reader {\n",
    "         input_path: \"path/to/val.record\"\n",
    "       }\n",
    "     }\n",
    "     ```\n",
    "\n",
    "2. **Label Map Path:**\n",
    "   - Update the path to the label map file:\n",
    "     ```protobuf\n",
    "     label_map_path: \"path/to/label_map.pbtxt\"\n",
    "     ```\n",
    "\n",
    "3. **Checkpoint Path:**\n",
    "   - Set the path to the pretrained model checkpoint for fine-tuning:\n",
    "     ```protobuf\n",
    "     fine_tune_checkpoint: \"path/to/checkpoint/ckpt-0\"\n",
    "     ```\n",
    "\n",
    "4. **Output Directory:**\n",
    "   - Define where training results should be saved using the `--model_dir` flag when running the training script.\n",
    "\n",
    "Ensure all paths are correct and accessible for successful training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Can you run Detectron2 on a CPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you can run Detectron2 on a CPU, but it will be significantly slower compared to using a GPU. To run on a CPU:\n",
    "\n",
    "1. **Install Detectron2:** Install Detectron2 without GPU-specific dependencies:\n",
    "   ```bash\n",
    "   pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu-none/torch.html\n",
    "   ```\n",
    "\n",
    "2. **Set Device:** Configure the model to use the CPU:\n",
    "   ```python\n",
    "   cfg.MODEL.DEVICE = \"cpu\"\n",
    "   ```\n",
    "\n",
    "3. **Run:** Execute your code as usual; it will process on the CPU.\n",
    "\n",
    "CPU execution is suitable for testing and small-scale experiments but is not recommended for large models or datasets due to its slow speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Why are label maps used in TFOD2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow Object Detection API (TFOD2), label maps are used to map class labels (such as \"cat\" or \"dog\") to unique integer IDs, which are essential for training and evaluating object detection models.\n",
    "\n",
    "**Key purposes of label maps:**\n",
    "\n",
    "1. **Class Identification:** They define the classes in your dataset, with each label assigned a unique integer ID.\n",
    "2. **Data Processing:** Label maps help the model understand and correctly associate object annotations (bounding boxes) with their corresponding class.\n",
    "3. **Compatibility:** They ensure that the training and evaluation scripts correctly reference and process class labels during model training.\n",
    "\n",
    "This file links each class with a specific ID used in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. What makes TFOD2 popular for real-time detection tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Object Detection API (TFOD2) is popular for real-time detection tasks due to several key factors:\n",
    "\n",
    "1. **Pretrained Models:** TFOD2 provides a wide range of pretrained models optimized for real-time performance, including models like SSD and Faster R-CNN, which balance speed and accuracy.\n",
    "\n",
    "2. **Efficiency:** Models such as SSD (Single Shot MultiBox Detector) and EfficientDet are designed for fast inference, making TFOD2 suitable for real-time applications on both CPU and GPU.\n",
    "\n",
    "3. **Flexibility:** It supports a variety of tasks, including object detection, instance segmentation, and keypoint detection, which can be fine-tuned for specific real-time use cases.\n",
    "\n",
    "4. **Integration with TensorFlow:** Since TFOD2 is built on TensorFlow, it leverages TensorFlowâ€™s optimization tools (like TensorRT and TensorFlow Lite) to accelerate inference for deployment on mobile and edge devices.\n",
    "\n",
    "5. **Ease of Use:** TFOD2 provides easy-to-follow scripts and an intuitive interface for training and deploying models, making it accessible for both research and production environments.\n",
    "\n",
    "These factors make TFOD2 a go-to solution for real-time object detection tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. How does batch size impact GPU memory usage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size directly impacts GPU memory usage because it determines how many samples are processed simultaneously during each training step.\n",
    "\n",
    "1. **Larger Batch Size:** \n",
    "   - **Increases GPU memory usage** as more data is loaded into memory at once.\n",
    "   - Requires more memory for storing activations, gradients, and intermediate computations.\n",
    "\n",
    "2. **Smaller Batch Size:** \n",
    "   - **Reduces GPU memory usage** because fewer samples are processed at once.\n",
    "   - Results in less memory consumption but may lead to slower convergence and noisier gradients.\n",
    "\n",
    "A larger batch size improves training speed but can cause memory overflow if the GPU doesnâ€™t have enough memory. Conversely, a smaller batch size conserves memory but might slow down training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. Whatâ€™s the role of Intersection over Union (IoU) in model evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection over Union (IoU) is a key metric in evaluating the performance of object detection models. It measures the overlap between the predicted bounding box and the ground truth bounding box.\n",
    "\n",
    "**Role of IoU in model evaluation:**\n",
    "\n",
    "1. **Quantifying Accuracy:** IoU calculates how well the predicted bounding box aligns with the actual object location in the image. Higher IoU values indicate better predictions.\n",
    "\n",
    "2. **Threshold for Detection:** IoU is used to set thresholds (e.g., 0.5 IoU) to determine if a prediction is a true positive or a false positive. If IoU exceeds the threshold, the prediction is considered correct.\n",
    "\n",
    "3. **Performance Metric:** In tasks like object detection, mean Average Precision (mAP) is calculated using IoU, helping to evaluate the model's overall detection quality across multiple classes and conditions.\n",
    "\n",
    "IoU is crucial for measuring both localization accuracy and model robustness in detecting objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. What is Faster R-CNN, and does TFOD2 support it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster R-CNN (Region Convolutional Neural Network) is an advanced object detection model that combines a Region Proposal Network (RPN) with a Fast R-CNN detector. The RPN generates potential object proposals (regions of interest), which are then processed by Fast R-CNN for classification and bounding box regression. This method significantly improves detection speed and accuracy compared to earlier models.\n",
    "\n",
    "**Key features of Faster R-CNN:**\n",
    "1. **Region Proposal Network (RPN):** Efficiently generates object proposals.\n",
    "2. **End-to-End Training:** Both the RPN and the detector are trained jointly.\n",
    "3. **High Accuracy:** It provides state-of-the-art performance in object detection tasks.\n",
    "\n",
    "**Support in TFOD2:**\n",
    "Yes, TensorFlow Object Detection API (TFOD2) supports Faster R-CNN. It provides pretrained Faster R-CNN models, which can be fine-tuned on custom datasets for object detection tasks. TFOD2 also offers several variations, such as Faster R-CNN with different backbone architectures (e.g., ResNet, Inception)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. How does Detectron2 use pretrained weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detectron2 uses pretrained weights to improve training efficiency and performance by leveraging models that have already been trained on large datasets, such as COCO. The process of using pretrained weights involves:\n",
    "\n",
    "1. **Model Initialization:** Detectron2 loads a pretrained model's weights into a model architecture, typically using a model from the Model Zoo (e.g., Faster R-CNN, Mask R-CNN).\n",
    "\n",
    "2. **Transfer Learning:** When training on a new dataset, these pretrained weights help the model learn faster by providing a strong feature extraction foundation. Only the last layers or specific parts may be fine-tuned for the new task.\n",
    "\n",
    "3. **Reduced Training Time:** By starting with pretrained weights, the model requires fewer epochs to converge, making the training process more efficient.\n",
    "\n",
    "Pretrained weights allow Detectron2 to achieve high performance with less data and computational resources, especially for tasks like object detection, instance segmentation, and keypoint detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. What file format is typically used to store training data in TFOD2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow Object Detection API (TFOD2), the typical file format used to store training data is **TFRecord**. \n",
    "\n",
    "**TFRecord** is a binary file format designed for efficient storage and retrieval of large datasets. It stores data in the form of serialized `tf.train.Example` protocol buffers, which can include images, labels, and other metadata.\n",
    "\n",
    "- **Advantages:** TFRecord files are optimized for TensorFlow's input pipeline, enabling fast data loading and processing during training.\n",
    "- **Conversion:** You can convert your dataset (e.g., COCO, Pascal VOC) into TFRecord format using scripts provided by TFOD2.\n",
    "\n",
    "TFRecord files are required for training models in TFOD2, where the images and their annotations (like bounding boxes and labels) are stored in a structured, efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. What is the difference between semantic segmentation and instance segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between **semantic segmentation** and **instance segmentation** lies in how objects are treated in an image:\n",
    "\n",
    "1. **Semantic Segmentation:**\n",
    "   - **Goal:** Classifies each pixel into a predefined class (e.g., \"car,\" \"tree,\" \"sky\").\n",
    "   - **Output:** Every pixel in the image is assigned a class label, but different instances of the same class (e.g., two cars) are not differentiated.\n",
    "   - **Example:** All pixels belonging to cars are labeled as \"car,\" without distinguishing between individual cars.\n",
    "\n",
    "2. **Instance Segmentation:**\n",
    "   - **Goal:** Identifies both the class and the specific instance of each object in the image.\n",
    "   - **Output:** In addition to class labels, each unique object instance (e.g., each individual car) is segmented and labeled separately.\n",
    "   - **Example:** Pixels corresponding to each individual car are labeled separately as different instances of \"car.\"\n",
    "\n",
    "**Summary:** Semantic segmentation labels pixels by class, while instance segmentation labels pixels by both class and object instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. Can Detectron2 detect custom classes during inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, **Detectron2** can detect custom classes during inference. To do this, you need to:\n",
    "\n",
    "1. **Train the Model with Custom Data:** Fine-tune a pretrained model (e.g., Faster R-CNN, Mask R-CNN) on your custom dataset with labeled classes.\n",
    "   \n",
    "2. **Update the Label Map:** Ensure that your custom classes are properly defined in a label map file (mapping class names to IDs).\n",
    "\n",
    "3. **Inference:** During inference, the model will predict and output bounding boxes, segmentation masks, and class labels corresponding to the custom classes based on the trained model.\n",
    "\n",
    "Detectron2 uses the class definitions from your training dataset to recognize and classify objects in new images during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. Why is pipeline.config essential in TFOD2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`pipeline.config`** file is essential in TensorFlow Object Detection API (TFOD2) because it contains the configuration settings for the model training and evaluation process. Key roles include:\n",
    "\n",
    "1. **Model Configuration:** Specifies the model architecture (e.g., Faster R-CNN, SSD) and hyperparameters (e.g., learning rate, batch size, number of steps).\n",
    "   \n",
    "2. **Data Paths:** Defines paths to the training and validation datasets (TFRecord files) and the label map, linking the data to the model.\n",
    "\n",
    "3. **Training Settings:** Configures aspects like optimizer settings, fine-tuning checkpoints, and evaluation metrics (e.g., mAP).\n",
    "\n",
    "4. **Performance Tuning:** Helps control model complexity, memory usage, and training speed, allowing adjustments based on available resources.\n",
    "\n",
    "In summary, the `pipeline.config` file centralizes all crucial settings for training a model, making it essential for proper configuration and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. What type of models does TFOD2 support for object detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Object Detection API (TFOD2) supports a variety of models for object detection, each suited for different trade-offs between speed, accuracy, and computational resources. Key model types include:\n",
    "\n",
    "1. **Faster R-CNN:** High accuracy, uses Region Proposal Networks (RPN) for generating object proposals. Suitable for applications where precision is critical.\n",
    "   \n",
    "2. **SSD (Single Shot Multibox Detector):** Faster than Faster R-CNN, with a good balance of speed and accuracy. Ideal for real-time detection tasks.\n",
    "\n",
    "3. **RetinaNet:** A one-stage detector that uses focal loss to address class imbalance, providing a balance between speed and accuracy.\n",
    "\n",
    "4. **EfficientDet:** A highly efficient model that offers good performance with fewer computational resources, ideal for mobile and edge devices.\n",
    "\n",
    "5. **YOLO (You Only Look Once):** A fast, single-stage detector for real-time object detection tasks, available through integration in TFOD2.\n",
    "\n",
    "These models can be fine-tuned on custom datasets to suit specific object detection needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33. What happens if the learning rate is too high during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning rate is too high during training, it can lead to several issues:\n",
    "\n",
    "1. **Overshooting Optimal Solution:** The model may take too large steps during weight updates, causing it to overshoot the optimal solution and fail to converge.\n",
    "   \n",
    "2. **Instability:** Training may become unstable, with the loss fluctuating wildly or even increasing, as the model cannot find a stable path towards the minimum.\n",
    "\n",
    "3. **Poor Performance:** The model might never reach the optimal or a near-optimal solution, resulting in poor performance on both training and validation data.\n",
    "\n",
    "In such cases, reducing the learning rate or using learning rate schedules can help stabilize and improve the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34. What is COCO JSON format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **COCO JSON format** is a widely-used annotation format for object detection, segmentation, and keypoint detection tasks. It stores detailed information about images, annotations, and object categories in a structured JSON file. \n",
    "\n",
    "Key components include:\n",
    "\n",
    "1. **Images:** Metadata such as image ID, file name, and dimensions.\n",
    "2. **Annotations:** Contains information about object bounding boxes, segmentation masks, and keypoints, along with corresponding image IDs.\n",
    "3. **Categories:** Lists the object classes (e.g., \"person,\" \"car\") with unique IDs and names.\n",
    "4. **Licenses & Info:** Additional metadata like license type and dataset details.\n",
    "\n",
    "This format facilitates easy integration with deep learning frameworks like TensorFlow and Detectron2 and is commonly used for training object detection models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. Why is TensorFlow Lite compatibility important in TFOD2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Lite compatibility** is important in TensorFlow Object Detection API (TFOD2) because it enables the deployment of object detection models on **mobile devices** and **edge devices** with **limited computational resources**. Key benefits include:\n",
    "\n",
    "1. **Model Optimization:** TensorFlow Lite reduces model size and improves inference speed, making it feasible to run object detection models on mobile and embedded systems.\n",
    "   \n",
    "2. **Efficiency:** It ensures efficient use of CPU, GPU, and specialized hardware (e.g., Edge TPUs) for real-time, low-latency predictions.\n",
    "\n",
    "3. **Cross-Platform Support:** TFOD2 models can be easily converted to TensorFlow Lite format for deployment across a wide range of devices, including smartphones, IoT devices, and embedded systems.\n",
    "\n",
    "In summary, TensorFlow Lite compatibility allows TFOD2 models to be optimized for and run efficiently on resource-constrained devices, enabling real-time object detection in practical, mobile, and embedded applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How do you install Detectron2 using pip and check the version of Detectron2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch1.13/index.html\n",
    "\n",
    "import detectron2\n",
    "print(detectron2.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How do you perform inference with Detectron2 using an online image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Step 1: Download the image from a URL\n",
    "url = \"https://wallup.net/wp-content/uploads/2016/01/211594-nature-landscape.jpg\"  # Replace with your image URL\n",
    "response = requests.get(url)\n",
    "image = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Step 2: Set up the Detectron2 configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.DEVICE = \"cuda\"  # Use \"cpu\" if you don't have a GPU\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Step 3: Perform inference\n",
    "outputs = predictor(image)\n",
    "\n",
    "# Step 4: Visualize the results\n",
    "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "result_image = v.get_image()[:, :, ::-1]\n",
    "\n",
    "# Step 5: Display the output image\n",
    "cv2.imshow(\"Inference Result\", result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How do you visualize evaluation metrics in Detectron2, such as training loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine import DefaultPredictor\n",
    "import tensorboard\n",
    "\n",
    "# Step 1: Install and Import Required Libraries\n",
    "# Install the required libraries:\n",
    "# pip install detectron2 requests opencv-python tensorboard\n",
    "\n",
    "# Step 2: Set up configuration and model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.DEVICE = \"cuda\"  # Use \"cpu\" if no GPU available\n",
    "\n",
    "# Step 3: Define output directory for logging\n",
    "cfg.OUTPUT_DIR = \"./output\"  # Directory where logs will be saved\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Step 4: Set up trainer and start training\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "\n",
    "# Training the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 5: Visualizing with TensorBoard\n",
    "# After training starts, open a terminal and run the following command:\n",
    "# tensorboard --logdir=output\n",
    "# Then open your browser and navigate to http://localhost:6006 to view the training logs and metrics.\n",
    "\n",
    "# Step 6: Perform inference with an online image\n",
    "# Download an image from a URL and perform inference\n",
    "url = \"https://wallup.net/wp-content/uploads/2016/01/211594-nature-landscape.jpg\"  # Replace with your image URL\n",
    "response = requests.get(url)\n",
    "image = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Initialize the model with the trained weights for inference\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Perform inference\n",
    "outputs = predictor(image)\n",
    "\n",
    "# Visualize the results\n",
    "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "result_image = v.get_image()[:, :, ::-1]\n",
    "\n",
    "# Display the inference result\n",
    "cv2.imshow(\"Inference Result\", result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How do you run inference with TFOD2 on an online image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import cv2\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from io import BytesIO\n",
    "\n",
    "# Step 1: Load the Pretrained Model\n",
    "# Load the model from a pre-trained model checkpoint (replace with your custom model path if needed)\n",
    "MODEL_NAME = 'ssd_inception_v2_coco_2017_11_17'  # Example pre-trained model\n",
    "PATH_TO_CKPT = f'http://download.tensorflow.org/models/object_detection/{MODEL_NAME}.tar.gz'\n",
    "\n",
    "# Download and extract the model (for online access)\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "response = requests.get(PATH_TO_CKPT)\n",
    "with open('model.tar.gz', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "with tarfile.open('model.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./')\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.saved_model.load('./ssd_inception_v2_coco_2017_11_17/saved_model')\n",
    "\n",
    "# Step 2: Load Label Map\n",
    "LABEL_MAP_PATH = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH, use_display_name=True)\n",
    "\n",
    "# Step 3: Function to Load and Prepare the Image\n",
    "def load_image_into_numpy_array(url):\n",
    "    response = requests.get(url)\n",
    "    image_data = np.array(bytearray(response.content), dtype=np.uint8)\n",
    "    image = cv2.imdecode(image_data, cv2.IMREAD_COLOR)  # Decode image into an array\n",
    "    return image\n",
    "\n",
    "# Step 4: Run Inference\n",
    "def run_inference(image_path):\n",
    "    image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "    # The input needs to be a tensor, so we convert the image to a tensor\n",
    "    input_tensor = tf.convert_to_tensor(image_np)\n",
    "    input_tensor = input_tensor[tf.newaxis,...]  # Add batch dimension\n",
    "\n",
    "    # Run detection\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches of detections, so we take the first one.\n",
    "    output_dict = {key:value.numpy() for key,value in output_dict.items()}\n",
    "\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'][0],\n",
    "        output_dict['detection_classes'][0].astype(np.int32),\n",
    "        output_dict['detection_scores'][0],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "\n",
    "    return image_np\n",
    "\n",
    "# Step 5: Visualize Results\n",
    "# Example: URL of an online image\n",
    "image_url = 'https://example.com/your_image.jpg'  # Replace with your image URL\n",
    "\n",
    "result_image = run_inference(image_url)\n",
    "\n",
    "# Step 6: Display the result\n",
    "cv2.imshow('Detection Result', result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How do you install TensorFlow Object Detection API in Jupyter Notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow\n",
    "!pip install tensorflow\n",
    "\n",
    "# Install dependencies for TensorFlow Object Detection API\n",
    "!pip install tf-slim\n",
    "!pip install tensorflow-hub\n",
    "!pip install tensorflow-graphics\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n",
    "\n",
    "# Install the Object Detection API\n",
    "!pip install --upgrade pip\n",
    "!pip install setuptools==59.5.0\n",
    "!pip install tensorflow-object-detection-api\n",
    "# Install dependencies for building TensorFlow Object Detection API from source\n",
    "!pip install pillow lxml Cython contextlib2 jupyter\n",
    "!pip install matplotlib pandas opencv-python tf-slim\n",
    "\n",
    "# Clone the TensorFlow models repository (if not already available)\n",
    "!git clone https://github.com/tensorflow/models.git\n",
    "\n",
    "# Navigate to the 'models' directory\n",
    "%cd models/research/\n",
    "\n",
    "# Install the Object Detection API from source\n",
    "!python setup.py install\n",
    "\n",
    "\n",
    "# Test the installation by importing the Object Detection API\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Object Detection API is installed successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How can you load a pre-trained TensorFlow Object Detection model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import model_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# URL of the pre-trained model from TensorFlow Model Zoo\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "MODEL_PATH = 'http://download.tensorflow.org/models/object_detection/' + MODEL_NAME + '.tar.gz'\n",
    "\n",
    "# Download and extract the model\n",
    "import tarfile\n",
    "import os\n",
    "import requests\n",
    "\n",
    "response = requests.get(MODEL_PATH)\n",
    "with open('model.tar.gz', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract the downloaded tar file\n",
    "with tarfile.open('model.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./')\n",
    "\n",
    "# Get the path to the saved_model directory\n",
    "PATH_TO_SAVED_MODEL = './ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model'\n",
    "# Load the saved model\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "# Load label map for COCO\n",
    "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "# Load an image from file (replace with the image path or URL)\n",
    "image_path = 'your_image.jpg'  # Replace with your image file path\n",
    "image_np = cv2.imread(image_path)\n",
    "image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "# Convert image to tensor\n",
    "input_tensor = tf.convert_to_tensor(image_np)\n",
    "input_tensor = input_tensor[tf.newaxis,...]  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "output_dict = detect_fn(input_tensor)\n",
    "\n",
    "# The output dictionary contains:\n",
    "# 'detection_boxes', 'detection_scores', 'detection_classes', 'num_detections'\n",
    "output_dict = {key:value.numpy() for key,value in output_dict.items()}\n",
    "\n",
    "# Visualize the results\n",
    "vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "    image_np,\n",
    "    output_dict['detection_boxes'][0],\n",
    "    output_dict['detection_classes'][0].astype(np.int32),\n",
    "    output_dict['detection_scores'][0],\n",
    "    category_index,\n",
    "    instance_masks=output_dict.get('detection_masks', None),\n",
    "    use_normalized_coordinates=True,\n",
    "    line_thickness=8\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Detection Result', cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How do you preprocess an image from the web for TFOD2 inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow opencv-python numpy requests\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import cv2\n",
    "\n",
    "def preprocess_image_from_web(image_url, target_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess an image from a URL for TFOD2 inference.\n",
    "\n",
    "    Args:\n",
    "        image_url (str): The URL of the image to download and preprocess.\n",
    "        target_size (tuple): Target size for resizing the image, e.g., (320, 320). If None, no resizing is done.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Preprocessed image tensor with batch dimension.\n",
    "        np.array: Original image as a NumPy array (RGB format) for visualization.\n",
    "    \"\"\"\n",
    "    # Step 1: Download the image\n",
    "    response = requests.get(image_url)\n",
    "    image_data = np.array(bytearray(response.content), dtype=np.uint8)\n",
    "\n",
    "    # Step 2: Decode the image\n",
    "    image = cv2.imdecode(image_data, cv2.IMREAD_COLOR)  # Decode to BGR format\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)      # Convert to RGB format\n",
    "\n",
    "    # Step 3: Resize the image (if target_size is provided)\n",
    "    if target_size:\n",
    "        image = cv2.resize(image, target_size)\n",
    "\n",
    "    # Step 4: Normalize pixel values (if required by your model)\n",
    "    # image = image / 255.0  # Uncomment if the model requires normalization\n",
    "\n",
    "    # Step 5: Convert to TensorFlow tensor and add batch dimension\n",
    "    input_tensor = tf.convert_to_tensor(image, dtype=tf.uint8)  # Use dtype=tf.float32 if normalized\n",
    "    input_tensor = tf.expand_dims(input_tensor, axis=0)  # Add batch dimension\n",
    "\n",
    "    return input_tensor, image\n",
    "\n",
    "# Example usage:\n",
    "image_url = 'https://wallup.net/wp-content/uploads/2016/01/211594-nature-landscape.jpg'  # Replace with your image URL\n",
    "target_size = (320, 320)  # Replace with the size required by your model\n",
    "\n",
    "input_tensor, original_image = preprocess_image_from_web(image_url, target_size)\n",
    "\n",
    "print(f\"Preprocessed Tensor Shape: {input_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How do you visualize bounding boxes for detected objects in TFOD2 inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "def visualize_detections(image, output_dict, category_index):\n",
    "    \"\"\"\n",
    "    Visualizes bounding boxes on an image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The original image in RGB format.\n",
    "        output_dict (dict): Model output containing detection boxes, classes, and scores.\n",
    "        category_index (dict): Dictionary mapping class IDs to class names.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with bounding boxes drawn.\n",
    "    \"\"\"\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'].astype(np.int32),\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=5\n",
    "    )\n",
    "    return image\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example detection output (replace with actual output from model inference)\n",
    "    output_dict = {\n",
    "        'detection_boxes': np.array([[0.1, 0.1, 0.5, 0.5], [0.6, 0.6, 0.9, 0.9]]),  # [ymin, xmin, ymax, xmax]\n",
    "        'detection_classes': np.array([1, 3]),  # Class IDs (e.g., person, car, etc.)\n",
    "        'detection_scores': np.array([0.95, 0.8])  # Confidence scores\n",
    "    }\n",
    "\n",
    "    # Load an image (replace with your image path)\n",
    "    image_path = 'example_image.jpg'  # Replace with the actual image path\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "    # Load category index (e.g., COCO label map)\n",
    "    label_map_path = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "    category_index = label_map_util.create_category_index_from_labelmap(label_map_path, use_display_name=True)\n",
    "\n",
    "    # Visualize detections\n",
    "    image_with_detections = visualize_detections(image, output_dict, category_index)\n",
    "\n",
    "    # Display the image with detections\n",
    "    image_with_detections = cv2.cvtColor(image_with_detections, cv2.COLOR_RGB2BGR)  # Convert back to BGR for OpenCV\n",
    "    cv2.imshow('Detections', image_with_detections)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. How do you define classes for custom training in TFOD2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. How do you resize an image before detecting object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def resize_image_tf(image, target_size):\n",
    "    \"\"\"\n",
    "    Resize an image to the specified dimensions using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        image (tf.Tensor): Input image tensor in the format [height, width, channels].\n",
    "        target_size (tuple): Target size as (height, width).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Resized image tensor.\n",
    "    \"\"\"\n",
    "    resized_image = tf.image.resize(image, target_size)\n",
    "    return resized_image\n",
    "\n",
    "# Example Usage\n",
    "image = tf.random.uniform(shape=(480, 640, 3), minval=0, maxval=255, dtype=tf.float32)  # Dummy image\n",
    "target_size = (320, 320)\n",
    "resized_image = resize_image_tf(image, target_size)\n",
    "print(f\"Resized Image Shape: {resized_image.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. How can you apply a color filter (e.g., red filter) to an image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def apply_red_filter(image):\n",
    "    \"\"\"\n",
    "    Applies a red filter to an image by enhancing the red channel.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image in BGR format.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with a red filter applied.\n",
    "    \"\"\"\n",
    "    # Split the image into its color channels (B, G, R)\n",
    "    blue, green, red = cv2.split(image)\n",
    "    \n",
    "    # Set the blue and green channels to zero\n",
    "    blue[:] = 0\n",
    "    green[:] = 0\n",
    "    \n",
    "    # Merge the channels back\n",
    "    red_filtered_image = cv2.merge((blue, green, red))\n",
    "    return red_filtered_image\n",
    "\n",
    "# Example Usage\n",
    "image = cv2.imread('example_image.jpg')  # Load image\n",
    "red_filtered_image = apply_red_filter(image)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Red Filtered Image', red_filtered_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
